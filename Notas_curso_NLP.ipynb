{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas día 3\n",
    "\n",
    "* Ahora está explicando gradiente descendiente para redes neuronales feed-forward\n",
    "\n",
    "* Conexión residual: le paso la salida de la sigmoid \n",
    "\n",
    "* Funciones de activación. sigmoid, tanh, ReLu.\n",
    "\n",
    "* Técnicas de regularización. L1, L2, dropout\n",
    "\n",
    "* Inicialización de los parámetros. \n",
    "\n",
    "\n",
    "## Modelo de lenguaje:\n",
    "\n",
    "Dado un contexto, quiero predecir qué palabra viene después. Siempre termino estimando probabilidades.\n",
    "\n",
    "N-gramas:\n",
    "\n",
    "$$\n",
    "P(x_n|x_{n-1}\\ldots x_1) = \\frac{frec(x_{n}\\ldots x_{1})}{frec(x_{n-1}\\ldots x_{1})}\n",
    "$$\n",
    "\n",
    "$x_i$ es el evento que ocurrió la palabra $i$ de mi vocabulario. Esto no tiene en cuenta el sentido de la oración.\n",
    "\n",
    "Esto tiene algunos problemas que para solucionarlo con RNNs.\n",
    "\n",
    "## RNN\n",
    "\n",
    "* Le vamos a dar a la red una memoria en el tiempo. $h_t = f(h_{t-1},x_t) = \\sigma(W_h h_{t-1} + W_x x_t + b)$ y estas redes las usa para hacer un modelo de lenguaje.\n",
    "\n",
    "* El backpropagation se hace en el tiempo también\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "El problema de las RNN es que\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_t}{\\partial h_{t-k}} \\alpha (W_t^T)^k\n",
    "$$\n",
    "\n",
    "con lo cual el gradiente depende de los autovalores de la matriz $W_t$. Gradiente explosivo y vanishing gradient.\n",
    "\n",
    "* Perplexity como función de costos.\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esto es importante:\n",
    "\n",
    "El modo de trabajo que da buenos resultados en este momento consiste en aprender en forma no supervisada la representación de las palabras a partir de muchos datos y después se busca aplicar el modelo con pocos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas día 5: la posta\n",
    "\n",
    "## Transformers:\n",
    "\n",
    "Tengo un conjunto $X$ de word embeddings (varias palabras) y defino:\n",
    "\n",
    "* $q$ con una transformación $W_q$ (consulta)\n",
    "\n",
    "* $K$ con una tasnformación $W_k$ (claves)\n",
    "\n",
    "* $V$ con una transformación $W_v$ (valores)\n",
    "\n",
    "\n",
    "Contruyo un vector de atención $softmax(K^T q)$ y voy a utilizarlo para ponderar sobre $V$ para obtener $z$. O sea:\n",
    "\n",
    "$$\n",
    "z = V\\;softmax(K^T q)^T \n",
    "$$\n",
    "\n",
    "![alt text](self-attention.png)\n",
    "\n",
    "En este caso, $q$ lo defino a partir de la palabra que quiero tomar como \"importante\" (por ejemplo, quiero saber en \"el auto estacionó en el garage\" cuál es el sujeto de \"estacionó\", por lo que ese $q$ está definido para el embedding de \"estacionó\").\n",
    "\n",
    "Esto lo hago para cada word-embedding y obtengo un $Z$. Hago eso $h$ veces y todos los $Z$s los trasnformo en una matriz. Eso lo paso por una feed-froward y obtengo $H$, la matriz final. Esta es toda la trasnformación que se busca.\n",
    "\n",
    "**Base de datos GLUE: la filosofía de estos tipos es incentivar a la comunidad científica a que diseñen un sistema que se encargue de hacer una comprensión del lenguaje en general, no de resolver una tarea en particular.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP:\n",
    "\n",
    "* Inferencia del lenguaje natural\n",
    "\n",
    "* dataset SNLI: dadas dos frases, A(premisa) B(hipótesis), tengo que calsificar en 3 clases: neutrales, consecuentia, contradictorias.\n",
    "\n",
    "* Hay que reproducir los resultados del paper.  (Gururangan et al. 2018)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
