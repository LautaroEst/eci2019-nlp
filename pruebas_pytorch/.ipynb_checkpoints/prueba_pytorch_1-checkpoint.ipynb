{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8.0, 4.0)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creo tensores con Tensor, arange, randn, etc.\n",
    "* cat =  concatenate\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[[1,2,3],\n",
    "                   [4,5,6]],\n",
    "                  [[7,8,9],\n",
    "                   [10,11,12]]])\n",
    "\n",
    "b = torch.arange(10).view(2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational graph. En este ejemplo defino una función s = (x + y).sum() y calculo el gradiente con respecto a x e y.\n",
    "\n",
    "La moraleja es que necesito hacer una cadena de torch.autograd.Variable, que esas son las que saben las relaciones con las otras variables y pueden calcular el gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.autograd.Variable( torch.Tensor([1., 2., 3]), requires_grad=True )\n",
    "y = torch.autograd.Variable( torch.Tensor([4., 5., 6]), requires_grad=True )\n",
    "s = (x + y).sum()\n",
    "s.backward() # Esto calcula el gradiente\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([5., 7., 9.])\n",
      "<AddBackward0 object at 0x7fe699aba470>\n"
     ]
    }
   ],
   "source": [
    "# Variables wrap tensor objects\n",
    "x = torch.autograd.Variable( torch.Tensor([1., 2., 3]), requires_grad=True )\n",
    "# Crea una variable para calcular el gradiente\n",
    "\n",
    "# You can access the data with the .data attribute\n",
    "print (x.data)\n",
    "\n",
    "# You can also do all the same operations you did with tensors with Variables.\n",
    "y = torch.autograd.Variable( torch.Tensor([4., 5., 6]), requires_grad=True )\n",
    "z = x + y\n",
    "print (z.data) # Contiene el tensor guardado en la variable\n",
    "\n",
    "# BUT z knows something extra.\n",
    "print (z.grad_fn) # esto dice cómo calcular el gradiente\n",
    "\n",
    "s = z.sum()\n",
    "print (s)\n",
    "print (s.grad_fn)\n",
    "print(s.backward()) # calling .backward() on any variable will run backprop, starting from it.\n",
    "print (z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5100, -0.3137,  0.1744, -0.9533,  0.4755])\n",
      "tensor([0.1330, 0.1618, 0.2636, 0.0854, 0.3563])\n",
      "tensor(1.0000)\n",
      "tensor([-2.0176, -1.8213, -1.3333, -2.4609, -1.0321])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Ejemplo con Linar + activación\n",
    "lin = torch.nn.Linear(5, 3) # maps from R^5 to R^3, parameters A, b\n",
    "data = torch.randn(2, 5) # data is 2x5.\n",
    "F.relu(lin(data)) # ReLu(lin * data)\n",
    "\n",
    "# Softmax is also in torch.functional\n",
    "data = torch.randn(5)\n",
    "print (data)\n",
    "print (F.softmax(data, dim=0))\n",
    "print (F.softmax(data, dim=0).sum()) # Sums to 1 because it is a distribution!\n",
    "print (F.log_softmax(data, dim=0)) # theres also log_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plantilla para probar un modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo a utilizar:\n",
    "\n",
    "class LogisticRegressionClassifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Inicialización\n",
    "        pass\n",
    "    \n",
    "    def forward(self):\n",
    "        \n",
    "        # Defino la operación del modelo\n",
    "        pass\n",
    "    \n",
    "\n",
    "# Función de costo a utilizar:\n",
    "\n",
    "loss_function = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengo el conjunto de muestras-etiquetas: \n",
    "N = 10000\n",
    "N_train = (N * 2) // 3\n",
    "N_test = N - N_train\n",
    "d = 2\n",
    "X = torch.randn(N,d)\n",
    "X_train, X_test = X[:N_train,:], X[N_train:,:]\n",
    "data = [] # = [(m1, t1), (m2, t2), ..., (mN, tN)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "3 4\n",
      "5 6\n"
     ]
    }
   ],
   "source": [
    "# Defino el modelo que voy a usar:\n",
    "model = LogisticRegressionClassifier()\n",
    "\n",
    "# Defino el optimizador que voy a usar:\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1) # Para esto, tengo que definir la clase como una subclase  del otro módulo\n",
    "\n",
    "# Entrenamiento:\n",
    "N_epoch = 100 # Número de iteraciones\n",
    "for epoch in range(N_epoch):\n",
    "    for vector, target in data:\n",
    "        \n",
    "        # Forward pass:\n",
    "        model.zero_grad()\n",
    "        log_probs = model.forward(vector)\n",
    "        \n",
    "        # Cálculo del costo:\n",
    "        loss = loss_function(log_probs,target)\n",
    "        \n",
    "        # Backward pass:\n",
    "        loss.backward()\n",
    "        optimizer.step()      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
