{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen de NLP\n",
    "\n",
    "La idea es hacer un resumen de los conceptos que tienen que ver solamente con *Natural Language Processing*, no con machine learning.\n",
    "\n",
    "## *Bag of words*\n",
    "\n",
    "El método de *Bag of Words* dice que el significado de una oración se puede representar como un vector cuyos índices representan la cantidad de veces que aparecen las palabras de la oración. La disposición de los índices se hace a partir de un vocabulario fijo. Por ejemplo, yo tengo un vocabulario \n",
    "\n",
    "$$\n",
    "V=\\{ abeja, buena, casa, película, qué, que, zorro \\}\n",
    "$$ \n",
    "\n",
    "y puedo representar la oración \"Qué buena película\" como el vector\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix} 0 & 1 & 0 & 1 & 1 & 0 & 0\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar clasificación con este método:\n",
    "# Usar algún modelo que tome las palabras \n",
    "# del par de oraciones y lo clasifique en \"contradictoria\", \"consecuencia\" y \"neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Word embedding*\n",
    "\n",
    "La representación anterior del significado de una oración tiene el problema de que el vocabulario es generalmente grande, por lo que los vectores tienen una dimensión enorme y viven en un espacio muy alejado entre ellos. Para comprimir y aprovechar mejor la relación entre los vectores se usa el método de *word embedding*. En el método anterior la representación de cada palabra del vocabulario se hace a partir de un vector *one-hot*, pero ahora la idea es aprender una representación más compacta, en la que los vectores están más juntos si el significado es parecido y más separados si tienen significado distinto.\n",
    "\n",
    "![alt text](word_embedding1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar el pasaje de one-hot a word embedding con el problema que nos piden:\n",
    "# representar cada par de oraciones como una oración toda junta, y hacer que aprenda\n",
    "# una representación de word-embedding con un problema de clasificación en \n",
    "# contradictoria, neutral o consecuencia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación con N-grams\n",
    "\n",
    "Otra representación posible es que, en lugar de representar la oración con el promedio de sus word embeddings, lo representás con eso y sus N-grams. Es decir, ahora el vocabulario va a contener a las palabras \"que_buena\" y \"buena_película\". También se puede obtener más información todavía haciendo una división en N-grams de subpalabras. Pro ejemplo, \"bueniiisima\" la divido en \"bue\", \"uen\", \"eni\", \"nii\", \"iii\", \"iis\", \"isi\", \"sim\", \"ima\" y tomo eso como nuevas palabras.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
